{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 With Solutions (40 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 451: Machine Learning (Fall 2021)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark  -d -u -a 'Sebastian Raschka' -v -p numpy,scipy,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"paragraph\">\n",
    "  <p><br></p>\n",
    "  <p><br></p>\n",
    "  <p><br></p>\n",
    "  <p><br></p>\n",
    "  <p><br></p>\n",
    "  <p><br></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter Tuning and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will be working with a diabetes dataset which is available from OpenML (https://www.openml.org/d/37).\n",
    "\n",
    "\n",
    "The dataset contains information about 768 patients along with the Diabetes diagnosis. The Diabetes diagnosis is a binary label, where \"tested_positive\" means that a patient has diabetes and \"tested_negative\" means that a patient does not have diabetes.\n",
    "\n",
    "I additional to the class label, there are 8 numeric features in the dataset, which are listed below:\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Using Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1) Load the Dataset [5 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas to load the dataset from the `dataset_37_diabetes` file from OpenML. (Hint: I provided the correct link for that, but you need to change something in the `read_csv` default code to load it correctly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('https://www.openml.org/data/get_csv/37/dataset_37_diabetes.arff', # YOUR CODE\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2) Preprocess the class label [5 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the class label using pandas `apply` or `map` method. The mapping should be as follows:\n",
    "\n",
    "- `'tested_positive'` should be converted to `1`\n",
    "- `'tested_negative'` should be converted to `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "df['class'] = df['class'].#YOUR CODE\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3) Split dataset into training and test sets [5 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the dataset into 70% training and 30% test data\n",
    "- Perform a stratified split\n",
    "- use `0` as the random seed for shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "y = df['class'].values\n",
    "X = df.iloc[:, :-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4) Gridsearch and model selection [5 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to use `GridSearchCV` from scikit-learn to find the best parameters for `max_depth` and `criterion` for a decision tree. For max_depth, the values `[1, 2, 3, 4, 5, 10, 15, 20, None]` should be tried, and for criterion both Gini and Entropy should be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "param_grid = # YOUR CODE\n",
    "\n",
    "gs = GridSearchCV(estimator=tree,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Accuracy: %.2f%%' % (gs.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, print the best hyperparameters obtained from the `GridSearchCV` run. Also, compute the accuracy the model, which uses the best hyperparameter settings and was trained on the whole training set, on the test set (`X_test`, `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "\n",
    "print('Best Params: %s' % gs.best_params_)\n",
    "\n",
    "## model is already fit to the whole training set because  we used `refit=True` in GridSearchCV\n",
    "print('Test Accuracy: %.2f%%' % ( # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are asked to compute the accuracy of the model from the previous exercise (1.1), using the train set (`X_train`, `y_train`), via different bootstrap methods. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Compare the Out-of-Bag, .632, and .632+ bootstrap approaches [5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing the bootstrap estimates and confidence intervals, you are going to use the `bootstrap_point632_score` function implemented in MLxtend: \n",
    "http://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/\n",
    "\n",
    "The accruacy should be the mean accuracy over the 200 bootstrap values that the `bootstrap_point632_score` method returns.\n",
    "\n",
    "- For this, use the best model you obtained from the previous exercise 1.1.4)\n",
    "- use 200 bootstrap rounds\n",
    "- set the random seed to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Out-of-bag Bootstrap:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "from mlxtend.evaluate import bootstrap_point632_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Compute Out-of-bag Bootstrap\n",
    "scores = bootstrap_point632_score(# YOUR CODE\n",
    "\n",
    "\n",
    "# Compute accuracy (average over the bootstrap rounds)\n",
    "acc = # YOUR CODE\n",
    "print('Accuracy: %.2f%%' % (100*acc))\n",
    "\n",
    "# Compute the 95% confidence interval around the accuracy estimate\n",
    "lower = # YOUR CODE\n",
    "upper = # YOUR CODE\n",
    "print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.632 Bootstrap:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "\n",
    "# Compute .632 Bootstrap\n",
    "scores = bootstrap_point632_score(# YOUR CODE\n",
    "\n",
    "\n",
    "# Compute accuracy (average over the bootstrap rounds)\n",
    "acc = # YOUR CODE\n",
    "print('Accuracy: %.2f%%' % (100*acc))\n",
    "\n",
    "# Compute the 95% confidence interval around the accuracy estimate\n",
    "lower = # YOUR CODE\n",
    "upper = # YOUR CODE\n",
    "print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute .632+ Bootstrap:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "\n",
    "# Compute .632+ Bootstrap\n",
    "scores = bootstrap_point632_score(# YOUR CODE\n",
    "\n",
    "\n",
    "# Compute accuracy (average over the bootstrap rounds)\n",
    "acc = # YOUR CODE\n",
    "print('Accuracy: %.2f%%' % (100*acc))\n",
    "\n",
    "# Compute the 95% confidence interval around the accuracy estimate\n",
    "lower = # YOUR CODE\n",
    "upper = # YOUR CODE\n",
    "print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 Analyzing the different bootstrap results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) **[5 Pts]** Based on what you have learned it class, which of the three bootstrap methods (out-of-bag, 0.632, or 0.632+) do you expect to yield a generalization accuracy estimate from the training set that is closest to the true generalization performance of the model? Explain your reasoning in 1-3 sentences. (Tip: Think about optimistic and pessimistic bias).\n",
    "\n",
    "\n",
    "< YOUR ANSWER >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2) **[5 Pts]** Based on your observations from the experiment in 1.2.1), which bootstrap approach (out-of-bag, 0.632, or 0.632+) yields an accuracy estimate from the training dataset that is closest to the test set accuracy from exercise 1.1.4? Is this reasonable? Explain your answer in 1-3 sentences. Also, to answer this question, assume that the test set accuracy from 1.1.4) is a perfect estimate of the true generalization accuracy of the model. \n",
    "\n",
    "< YOUR ANSWER >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3) **[5 Pts]** Based on your observations from the experiment in 1.2.1), are the overall results consistent with what you expected in your answer above (question 1))? Explain your reasoning in 3-5 sentences. Also, to answer this question, assume that the test set accuracy from 1.1.4) is a perfect estimate of the true generalization accuracy of the model. \n",
    "\n",
    "Tip: Discuss which methods are optimistically and pessimistically biased and whether this was expected.\n",
    "\n",
    "< YOUR ANSWER >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
